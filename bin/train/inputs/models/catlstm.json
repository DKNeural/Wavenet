{
    "_comments": [
        "Parametric extension of the LSTM model. All LSTM tips apply plus:",
        " * Make sure that `input_size` is the number of knobs plus one. I've set it",
        "   up like we're modeling a tube screamer (drive/tone/level), so 1+3=4.",
        " * Doesn't seem like the model needs to be all that bigger than the",
        "   non-parametric version, even if you're modeling a fair number of knobs.",
        " * You'll probably have a much larger dataset, so validating every so often ",
        "   in steps instead of epochs helps. Make sure to also set val_check_interval",
        "   under the trainer dict in your learning config JSON.",
        "",
        "Dev note: Ensure that tests/test_bin/test_train/test_main.py's data is ",
        "representative of this!"
    ],
    "net": {
        "name": "CatLSTM",
        "config": {
            "num_layers": 4,
            "hidden_size": 32,
            "train_burn_in": 4096,
            "train_truncate": 512,
            "input_size": 4
        }
    },
    "loss": {
        "val_loss": "mse",
        "mask_first": 4096,
        "pre_emph_weight": 1.0,
        "pre_emph_coef": 0.85
    },
    "optimizer": {
        "lr": 0.01
    },
    "lr_scheduler": {
        "class": "ExponentialLR",
        "kwargs": {
            "gamma": 0.995
        },
        "interval": "step",
        "frequency": 100
    }
}